{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6 - Classifiers Boosting Algorithms\n",
    "\n",
    "In this lab, we will implement the AdaBoost algorithm as an ensemble learning technique which\n",
    "aims to combine a number of weak classifiers to yield a strong classifier at the end.\n",
    "The idea of this lab is to identify whether a tumor with given characteristics is malignant or\n",
    "benign. This is a two-class classification problem.\n",
    "\n",
    "## Dataset and Features\n",
    "\n",
    "You will be working on the dataset from *Hastie et al,* for breast tumor classification with 10 features representing the tumor's:\n",
    "\n",
    "                              1. Area            6. Texture\n",
    "                              2. Perimeter       7. Symmetry\n",
    "                              3. Radius          8. Greyscale Level\n",
    "                              4. Compactness     9. Fractal Dimension\n",
    "                              5. Concavity      10. Coastline Approximation.\n",
    "There is one output variable which is diagnosis. It takes one of two values `+1` for malignant and `-1` for benign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "Why it is sometimes better to have the two class values `+1` and `-1` instead of `+1`\n",
    "and `0`?\\\n",
    "**HINT :** Think about the voting scheme at the end of the boosting algorithm. How can the class values\n",
    "affect this scheme?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Your answer: \\n        ==> The reason why it is sometimes better to have the two class values +1 and -1 instead of +1 and 0 in AdaBoost classifier\\n            is that AdaBoost uses a decision stump as its base classifier,\\n            which is a one-level decision tree that splits the data into two groups based on a single feature.\\n        ==> The decision stump can only output two values, so it makes sense to use +1 and -1 as class labels instead of +1 and 0.\\n            This way, the decision stump can output either +1 or -1, which can be used to make binary decisions.\\n\\n        ==> In addition, AdaBoost uses a weighted combination of decision stumps to make its final prediction.\\n            The weights are chosen such that the misclassified samples have higher weights in the next iteration.\\n            If we use +1 and 0, then we cannot distinguish between positive and negative samples that are misclassified as 0.\\n            However, if we use +1 and -1, then we can distinguish between positive and negative samples that are misclassified as -1.\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Your answer: \n",
    "        ==> The reason why it is sometimes better to have the two class values +1 and -1 instead of +1 and 0 in AdaBoost classifier\n",
    "            is that AdaBoost uses a decision stump as its base classifier,\n",
    "            which is a one-level decision tree that splits the data into two groups based on a single feature.\n",
    "        ==> The decision stump can only output two values, so it makes sense to use +1 and -1 as class labels instead of +1 and 0.\n",
    "            This way, the decision stump can output either +1 or -1, which can be used to make binary decisions.\n",
    "\n",
    "        ==> In addition, AdaBoost uses a weighted combination of decision stumps to make its final prediction.\n",
    "            The weights are chosen such that the misclassified samples have higher weights in the next iteration.\n",
    "            If we use +1 and 0, then we cannot distinguish between positive and negative samples that are misclassified as 0.\n",
    "            However, if we use +1 and -1, then we can distinguish between positive and negative samples that are misclassified as -1.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirement\n",
    "You are required to fill the function `adaboost_classifier(Y_train, X_train, Y_test, X_test, T, clf).`\\\n",
    "This function takes as parameters:\n",
    "\n",
    "| | |\n",
    "|:---|:-|\n",
    "| **Y_train**| The target values for the training set |\n",
    "| **X_train**| The input features for the training set.|\n",
    "| **Y_test**| The target values for the test set.|\n",
    "| **Y_train**| The input features for the training set.|\n",
    "| **T**| The number of iterations of the AdaBoost Algorithm.|\n",
    "| **clf**| The classifier to be used. (In our case, we are using a decision tree stump as a base classifier). You can use any other classifier.|\n",
    "\n",
    "This function should return two values:\n",
    "- The accuracy of the model on the training set.\n",
    "- The accuracy of the model on the test set.\n",
    "\n",
    "\n",
    "#### Fair Note:\n",
    "In the explanation video, we assumed that (T) is the number of models you want to fit. However, this is not always the case. You may have a model base (like here we have decision trees) and you are allowed to use as many of it as you can. So (T) here becomes the number of iterations where your goal is to enhance the performance with as few iterations as possible. \n",
    "\n",
    "Do not get confused:\n",
    "- If your case is you have T models only, we set T = number of models to fit.\n",
    "- If you are allowed to use as many models as you can (as many decision trees as you need), then T is the number of iterations to choose. In such case, T becomes a parameter controlled by the programmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports ##\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "\n",
    "from utils import get_accuracy, print_accuracy, plot_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** we prepared some utility functions to help you throughout the lab. please go and check the file *utils.py* and make sure you understand each function and know how to use it.\n",
    "\n",
    "### TODO: AdaBoost Implementation\n",
    "\n",
    "AdaBoost is an iterative algorithm that gives weights for the best classifier every iteration, updates weights of the data points, then repeats until convergence.\n",
    "\n",
    "The steps of the algorithm are:\n",
    "\n",
    "1. Initialize weights of the training examples:\n",
    "\n",
    "$$w_{m} = \\frac {1}{M}, m = 1,2,...M$$\n",
    "\n",
    "                                        M: number of training examples. \n",
    "\n",
    "2. For t=1 to $T$:\n",
    "\n",
    "    a) Select a classifier $h_{t}$ that best fits to the training data using weights $w_{m}$ of the training examples.\n",
    "\n",
    "    b) Compute error of $h_{t}$ as:\n",
    "$$err_{t} = \\frac {\\Sigma_{m=1}^{M} w_{m} \\phi (c_{m} \\neq h_{t}(x_{m}))}{\\Sigma_{m=1}^{M} w_{m}}$$\n",
    "\n",
    "    c) Compute weight of classifier:\n",
    "$$\\alpha_{t} = \\log (\\frac {1-err_{t}}{err_{t}} )$$\n",
    "\n",
    "    d) Update weights of wrongly classified examples:\n",
    "$$w_{m} = w_{m} * \\exp^{\\alpha_{t} \\phi (c_{m} \\neq h_{t}(x_{m}))}, \\space m = 1 ... M$$\n",
    "\n",
    "    e) Renormalize weights $w_{m}$\n",
    "\n",
    "\n",
    "\\\n",
    "3. Output: $C(x)= argmax_{k}\\space (\\space \\Sigma_{t=1}^{T} \\alpha_{t} * \\phi (h_{t}(x) = k)) \\space)$\n",
    "\n",
    "**Where** in step 2.B and 2.D, the $\\phi (y)$ function is called the *miss indicator* function that gives values:\n",
    "\n",
    "                                     1: if y is True\n",
    "                                     0: if y is False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost_classifier(Y_train, X_train, Y_test, X_test, T, clf):\n",
    "    \n",
    "    #TODO: FILL THE FUNCTION with the implementation as the steps above\n",
    "\n",
    "    # TODO [1]: Initialize weights\n",
    "    w = np.array([1/len(X_train) for x in X_train ])\n",
    "\n",
    "    ## TODO [2]:  Initialize the training and test data with empty array placeholders\n",
    "    #### Hint: what should be their shape?\n",
    "    \n",
    "    # predicted classes of the training examples\n",
    "    pred_train = np.zeros((T,len(Y_train) ))       \n",
    "    # predicted classes of the test examples  \n",
    "    pred_test = np.zeros((T,len(Y_test) ))          \n",
    "\n",
    "    ## TODO [3]: loop over the boosting iterations \n",
    "    for i in range(T): \n",
    "\n",
    "        # TODO [4]: Fit a classifier with the specific weights \n",
    "        ## TODO [4.A]: fit the classifier on the training data\n",
    "        #### Hint: search how sklearn.tree.DecisionTreeClassifier fits classifier on data\n",
    "        ### Hint: search for parameter weights in the fit matrix\n",
    "        clf.fit(X_train, Y_train, sample_weight=w)\n",
    "        \n",
    "        # TODO [4.B]: predict classes for the training data and test data\n",
    "        pred_train_i = clf.predict(X_train)\n",
    "        pred_test_i = clf.predict(X_test)\n",
    "        \n",
    "        # TODO [5]: calculate the miss Indicator function\n",
    "        miss = (pred_train_i != Y_train)\n",
    "\n",
    "        # TODO [6]: calculate the error for the current classifier (err_t)\n",
    "        err_t = np.sum(w * miss)/np.sum(w)\n",
    "\n",
    "        # TODO [7]: calculate current classifier weight (Alpha_t)\n",
    "        alpha_t = np.log((1-err_t)/err_t)\n",
    "\n",
    "        # TODO [8]: update the weights \n",
    "        w = w * np.exp(alpha_t * miss)\n",
    "\n",
    "        # TODO [9] Add to the overall predictions\n",
    "        pred_train[i] = pred_train_i * alpha_t\n",
    "        pred_test[i] = pred_test_i * alpha_t\n",
    "        \n",
    "    final_pred_train = np.sum(pred_train, axis=0)\n",
    "    final_pred_test = np.sum(pred_test, axis=0)\n",
    "\n",
    "    final_pred_train[final_pred_train >= 0], final_pred_train[final_pred_train < 0]= 1, -1\n",
    "    final_pred_test[final_pred_test >= 0], final_pred_test[final_pred_test < 0]= 1, -1 \n",
    "\n",
    "    # TODO [10]: Return error rate in train and test set\n",
    "    #### Hint: use function get_accuracy from utils.py\n",
    "    train_error = get_accuracy(final_pred_train, Y_train )\n",
    "    test_error = get_accuracy(final_pred_test, Y_test )\n",
    "    \n",
    "    print_accuracy((train_error* 100, test_error * 100))\n",
    "    \n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Boosted Classifier\n",
    "\n",
    "Now we will use the function you implemented to build a classifer.\\\n",
    "You will not change code here, only read the code below and run it to see how **AdaBoost** enhanced the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data ...\n",
      "Number of Iterations :  10\n",
      "Accuracy: Training: 66.3750 - Test: 66.7083\n",
      "Number of Iterations :  60\n",
      "Accuracy: Training: 82.8750 - Test: 83.0417\n",
      "Number of Iterations :  110\n",
      "Accuracy: Training: 86.3542 - Test: 86.3750\n",
      "Number of Iterations :  160\n",
      "Accuracy: Training: 87.6771 - Test: 87.5417\n",
      "Number of Iterations :  210\n",
      "Accuracy: Training: 89.5938 - Test: 89.8750\n",
      "Number of Iterations :  260\n",
      "Accuracy: Training: 90.4792 - Test: 90.1667\n",
      "Number of Iterations :  310\n",
      "Accuracy: Training: 91.4688 - Test: 91.3750\n",
      "Number of Iterations :  360\n",
      "Accuracy: Training: 91.8646 - Test: 91.6667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Computer_Eng\\Third Year\\Second_Term\\Neural Networks\\Lab 6 - AdaBoost Classifier\\utils.py:20: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  plot1.set_xticklabels(range(0, 450, 50))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### DO NOT CHANGE CODE ####\n",
    "\n",
    "## First, read the dataset\n",
    "x,y = make_hastie_10_2()\n",
    "df = pd.DataFrame(x)\n",
    "df['Y'] = y\n",
    "print('Reading Data ...')\n",
    "\n",
    "# Split into training and test set\n",
    "train, test = train_test_split(df, test_size=0.2) # this function shuffles the data points, and splits the data into\n",
    "                                                  # 80% training set and 20% test set (indicated by test_size=0.2)\n",
    "\n",
    "train, test = np.array(train), np.array(test)\n",
    "X_train, Y_train = train[:, :-1], train[:, -1]\n",
    "X_test, Y_test = test[:, :-1], test[:, -1]\n",
    "# Fit a simple decision tree first\n",
    "clf_tree = DecisionTreeClassifier(max_depth=1, random_state=1)\n",
    "\n",
    "# Fit Adaboost classifier using a decision tree as base estimator\n",
    "# Test with different number of iterations\n",
    "acc_train, acc_test = [],[]\n",
    "x_range = range(10, 410, 50)\n",
    "for i in x_range:\n",
    "    print('Number of Iterations : ' , i)\n",
    "    acc_i = adaboost_classifier(Y_train, X_train, Y_test, X_test, i, clf_tree)\n",
    "    acc_train.append(acc_i[0])\n",
    "    acc_test.append(acc_i[1])\n",
    "\n",
    "# Compare error rate vs number of iterations\n",
    "plot_accuracy(acc_train, acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "Justify why the plot is the way it is (is it increasing or decreasing? why? when does it flattens out?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    Your answer:\\n        ==> It's increasing. There is a drastic decrease in slope starting from 100 iterations.\\n            Going further the slope keeps decreasing until it becomes about 0 starting form 350 iterations.\\n\\n        ==> The accuracy keeps on improving with iterations because in each iteration the classifier learns from its mistakes\\n            in the previous iteration and is then weighted depending on the new resulting loss it computes \\n            (the smaller it is the larger the weight) hence, accuracy should naturally increase with iterations.\\n\\n        ==> Now as we can see, at some point it starts to decrease then flatten out which means that there is less to learn\\n            every iteration (at the beginning with 65 percent accuracy it had much to learn.) so its also natural for the \\n            accuracy improvement to slow down as we go. \\n\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Your answer:\n",
    "        ==> It's increasing. There is a drastic decrease in slope starting from 100 iterations.\n",
    "            Going further the slope keeps decreasing until it becomes about 0 starting form 350 iterations.\n",
    "\n",
    "        ==> The accuracy keeps on improving with iterations because in each iteration the classifier learns from its mistakes\n",
    "            in the previous iteration and is then weighted depending on the new resulting loss it computes \n",
    "            (the smaller it is the larger the weight) hence, accuracy should naturally increase with iterations.\n",
    "\n",
    "        ==> Now as we can see, at some point it starts to decrease then flatten out which means that there is less to learn\n",
    "            every iteration (at the beginning with 65 percent accuracy it had much to learn.) so its also natural for the \n",
    "            accuracy improvement to slow down as we go. \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "The number of iterations (T) is what we call a hyper parameter:\n",
    "   - Its value differs from model to model and from problem to problem.\n",
    "   - Its value is not learnt by time, it is set by the programmer.\n",
    "   \n",
    "Suggest ways to select the optimal T keeping in mind that:\n",
    "   - If T is too big, the training time is large (you loop for T times, each time takes a model to fit and this model might take hours to fit)\n",
    "   - If T is too small, the boosting might not reach the best values it can get.\n",
    "   \n",
    "   \n",
    "\n",
    "**HINT**: Look at the graph of number of iterations vs performance and search for elbow method. Try to understand it and explain what it does.\\\n",
    "**HINT**: There are other hyper-parameter selection techniques, search for them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    Your answer:\\n        ==> AdaBoost should be run for a long time, until it converges, and that 1,000 iterations should be enough.\\n            So assuming speed is not issue we can run it a large number of iterations without worrying.    \\n    \\n        ==> The elbow method is often used to select the number of clusters in k-means, we keep watching the loss function\\n            as it decreases with the number of clusters. \\n        ==> We can apply the same huerstic to find the number of iterations here.\\n            Stopping there makes sense because we don't get so much from continuing and we make the model more susceptible \\n            to overfitting all while paying the same amount of resources (computationa and time.)\\n\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Your answer:\n",
    "        ==> AdaBoost should be run for a long time, until it converges, and that 1,000 iterations should be enough.\n",
    "            So assuming speed is not issue we can run it a large number of iterations without worrying.    \n",
    "    \n",
    "        ==> The elbow method is often used to select the number of clusters in k-means, we keep watching the loss function\n",
    "            as it decreases with the number of clusters. \n",
    "        ==> We can apply the same huerstic to find the number of iterations here.\n",
    "            Stopping there makes sense because we don't get so much from continuing and we make the model more susceptible \n",
    "            to overfitting all while paying the same amount of resources (computationa and time.)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
